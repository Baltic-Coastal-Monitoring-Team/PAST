<img src="https://c5studio.pl/past/past-logo.png" width="450px">

# PAST – Perceptron as Similarity Transformation  
### A Machine Learning Approach for 3D Coordinate Frame Alignment

**PAST** is a Jupyter-based tool for learning three-dimensional similarity transformations between coordinate frames using a single-layer perceptron.  
The software reformulates the classical geodetic transformation into a transparent, gradient-based learning process, where perceptron weights directly represent rotation and scale, and biases represent translation.  
It serves both as a scientific and educational framework for analyzing coordinate transformations, noise propagation, and transformation stability under limited data availability.

---

## Key Features

- **Machine-learning-based transformation solver**  
  Learns 3D rotation, translation, and scale parameters using gradient descent (PyTorch backend).  
  Operates efficiently even with a small number of control points (e.g., 5 GCPs).  
  Unlike classical SVD-based methods, PAST is a **white-box learning approach** — you see how the model converges, which parameters change, and how noise affects the solution.

- **White-box architecture**  
  Full interpretability – perceptron weights map directly to geometric transformation coefficients.  
  All intermediate quantities (gradients, RMSE evolution, orthogonality checks) are explicitly accessible.  
  Perfect for understanding **why** a transformation succeeds or fails, not just if it does.

- **Multiple transformation variants for robust comparison**  
  - **PAST_raw**: raw learned mapping (gradient-based)
  - **PAST_SVD**: SVD-projected version enforcing perfect orthogonality (det=+1, R^T R=I)
  - **LSM_rigid (Kabsch)**: classical SVD baseline (rigid rotation + translation, no scale)
  - **LSM_sim (Umeyama)**: classical SVD baseline with isotropic scale
  - **HYB (Hybrid)**: rigid LSM baseline + learned residual correction  
  
  Each method is evaluated side-by-side on RMSE/RMST/NRMSE metrics, helping you choose the best variant for your geodetic task.

- **Integration with GeoPOINT**  
  Can directly process `.xlsx` files generated by **GeoPOINT**, using the `Original_Global` and `Transformed_Local` sheets.  
  Seamless workflow for coordinate transformation projects across Baltic coastal monitoring teams.

- **Standalone mode**  
  Accepts any paired `.xlsx` or `.csv` datasets containing corresponding 3D coordinates.  
  Flexible input — work with custom data sources or integrate with external pipelines.

- **Orthogonality & quality diagnostics**  
  Automatic checks for rotation matrix orthogonality at three tolerance levels:  
  - **Strict** (1e-6): Sub-mm precision, rigorous scientific work  
  - **Practical** (1e-4): cm–dm precision, standard GIS/cartography  
  - **Lenient** (1e-3): m-scale precision, overview maps  
  
  Reports practical metrics (rotation angle in degrees, position drift in meters at 10 km / 100 km scales) to help non-mathematicians understand transformation quality.

- **Visualization and validation**  
  - RMSE convergence plot (training dynamics)
  - Per-axis residual deviation bars (σx, σy, σz)  
  - 3D scatter comparison (predicted vs. reference coordinates)
  - Per-method comparison histograms and box plots (PAST_raw vs PAST_SVD)
  - Comprehensive statistical tables (RMST, NRMSE) across all methods
  - Optional educational appendix (1D perceptron learning analogy)

- **Complete data export**  
  - Transformation parameters (R, t) in CSV and JSON
  - Per-point transformed datasets (input + predicted + reference)
  - Rotation metrics (angle, drifts) automatically computed
  - Ready-to-use CSV files for GIS workflows or further analysis

---

## Methodology & Transformation Variants

### The PAST Approach: Why a Perceptron?

**PAST** reformulates coordinate transformation as a supervised learning problem:
- **Input (X)**: Coordinates in the local/source frame
- **Output (Y)**: Coordinates in the fixed/global frame  
- **Learner**: Single-layer perceptron trained via gradient descent  
- **Parameters**: Weights → rotation/scale; biases → translation

Unlike classical least-squares methods (which assume all information is available upfront),  
PAST models the transformation **iteratively**, revealing how solution quality improves with training iterations.  
This makes it ideal for understanding transformation behavior under noisy or incomplete data.

### Available Methods

PAST includes five transformation variants, automatically computed and compared:

#### 1. **PAST_raw** (Gradient-Learned Mapping)
- Raw perceptron output after training on GCPs
- **Pros**: Often has excellent training RMSE; captures nonlinearities in learned residuals
- **Cons**: May violate orthogonality (rotation matrix not perfectly orthonormal); can overfit small GCP sets
- **Best for**: Understanding learned patterns; validating that perceptron convergence is sensible

#### 2. **PAST_SVD** (Orthogonality-Corrected)
- PAST_raw matrix decomposed via SVD, then reconstructed as perfect similarity transform (sR with det(R)=+1)
- **Pros**: Enforces geometric constraints (perfect rotation + isotropic scale); often generalizes better to validation data
- **Cons**: May lose some training accuracy (small projection error ≈ cost of orthogonalization)
- **Best for**: Production-grade transformations; downstream GIS/surveying workflows

#### 3. **LSM_rigid (Kabsch Algorithm)**
- Classical SVD-based rigid alignment: **Y = R·X + t** (rotation + translation, no scale)
- **Pros**: Mathematically optimal for rigid transformations; low computational cost; stable baseline
- **Cons**: Assumes no scale difference; may underperform if coordinate systems differ in scale
- **Best for**: Validating PAST performance against classical methods; rigid-only tasks

#### 4. **LSM_sim (Umeyama Algorithm)**
- Classical SVD-based similarity: **Y = s·R·X + t** (rotation + translation + isotropic scale)
- **Pros**: Handles scale differences; SVD-guaranteed optimality for given GCP set
- **Cons**: Less flexible than PAST (no per-axis scaling); no learning dynamics visible
- **Best for**: Benchmark comparison; similarity-transform workflows

#### 5. **HYB (Hybrid: LSM_rigid + Learned Residuals)**
- Rigid LSM baseline + PAST-learned correction for residuals
- **Pros**: Combines stability of SVD baseline with learning flexibility; excellent generalization
- **Cons**: Two-stage process (less interpretable than single PAST_raw); moderate complexity
- **Best for**: High-precision applications; leveraging both classical and learned approaches

### Choosing the Right Variant

**Decision tree** (based on your requirements):

1. **Do you need maximum interpretability?**  
   → Use **PAST_raw** + examine orthogonality report and validation errors

2. **Do you need a production-grade transformation for GIS?**  
   → Use **PAST_SVD** (enforces orthogonality + good validation RMSE)

3. **Do you need classical SVD-optimal solution?**  
   → Use **LSM_rigid** (rigid) or **LSM_sim** (with scale)

4. **Do you want the best generalization to unseen data?**  
   → Compare **PAST_SVD** and **HYB** on validation RMSE/RMST

The notebook automatically computes all five and displays them side-by-side  
in Appendix 2, allowing you to **make an informed choice** based on your specific geodetic task.

### Quality Metrics Explained

Each variant is evaluated on:

- **RMSE (Root Mean Square Error)** per axis [m]  
  Residual prediction error in X, Y, Z directions separately

- **RMST (3D aggregate)** [m]  
  Spatial residual: $\text{RMST} = \sqrt{\text{RMSE}_x^2 + \text{RMSE}_y^2 + \text{RMSE}_z^2}$

- **NRMSE (Normalized RMSE)** [—]  
  RMSE divided by data range (scale-independent quality measure)

- **Orthogonality checks** (for PAST variants)  
  - Frobenius norm of (R^T R − I)  
  - Column norm deviations  
  - Determinant error |det(R) − 1|  
  → Three tolerance levels help you decide if the transformation is suitable for your task


---

## Installation

The repository provides a ready-to-use **`environment.yml`** file for Conda.

```bash
# Clone the repository
git clone https://github.com/Baltic-Coastal-Monitoring-Team/PAST.git
cd PAST

# Create environment
conda env create -f environment.yml

# Activate environment
conda activate past
```

**Environment overview**

The Conda environment includes:
-	Python 3.11
-	numpy, pandas, matplotlib, seaborn
-	scipy, sympy, scikit-learn
-	openpyxl (Excel import/export)
-	jupyterlab
-	torch (PyTorch backend for gradient learning)


## Quick Start

1. Launch JupyterLab:

```bash
jupyter lab
```

2. Open the **`PAST.ipynb`** notebook.  
3. In Section 1. Paths & Parameters, define your input file(s):

Option A – GeoPOINT mode

```python
GEOPPOINT_WORKBOOK = "geopoint_data/synthetic_with_noise_coordinates.xlsx"
```
Option B – External mode
```python
EXTERNAL_LOCAL_PATH = "data/local_frame.csv"
EXTERNAL_FIXED_PATH = "data/global_frame.csv"
```

4. Adjust parameters:
```python
N_GCP = 5        # number of points used for training
MAX_ITERS = 2000 # max iterations
INIT_LR = 0.01   # initial learning rate
```

5.	Run all cells sequentially.
6.	The notebook will generate:
-	Rotation matrix (R) and translation vector (t)
-	Training and validation RMSE
-	Orthogonality and determinant tests
-	Visual plots (RMSE, residuals, 3D scatter)
-	Results export to output_data/results_past.csv and results_past.json

## Repository Structure

```
PAST/
├─ PAST.ipynb                 # main notebook
├─ environment.yml            # conda environment
├─ geopoint_data/             # sample input from GeoPOINT
│   └─ synthetic_with_noise_coordinates.xlsx
├─ output_data/               # generated results
│   ├─ results_past.csv
│   ├─ results_past.json
│   └─ *_transformed.csv
└─ README.md                  # documentation
```

## Output Data Description

The notebook generates several output files in `output_data/` directory.  
Each provides a different view of the transformation results and is suitable for different use cases.

### Generated Files

| **File** | **Purpose** | **Format** | **Size** |
|----------|-----------|-----------|---------|
| `results_past.csv` | Summary of transformation parameters & metrics | CSV (1 row) | < 1 KB |
| `results_past.json` | Complete structured result record | JSON | < 10 KB |
| `PAST_raw_transformed.csv` | Per-point predictions (raw learned) | CSV (N rows) | depends on N |
| `PAST_SVD_transformed.csv` | Per-point predictions (SVD-corrected) | CSV (N rows) | depends on N |
| `LSM_rigid_transformed.csv` | Per-point predictions (Kabsch baseline) | CSV (N rows) | depends on N |
| `LSM_sim_transformed.csv` | Per-point predictions (Umeyama baseline) | CSV (N rows) | depends on N |
| `HYB_transformed.csv` | Per-point predictions (Hybrid) | CSV (N rows) | depends on N |

---

### `results_past.csv` — Summary Table

**Single-row table** containing all key parameters and metrics for the transformation run.

| **Column** | **Description** | **Units** | **Example** |
|------------|-----------------|-----------|------------|
| `R11`, `R12`, `R13` | First row of rotation matrix **R** | — | 0.8066 |
| `R21`, `R22`, `R23` | Second row of **R** | — | 0.5909 |
| `R31`, `R32`, `R33` | Third row of **R** | — | -0.0168 |
| `tX`, `tY`, `tZ` | Translation vector **t** | m | -2.37, -13.79, -2.92 |
| `detR` | Determinant of R (should be ≈ 1.0) | — | 0.99999 |
| `rotation_angle_deg` | Estimated rotation error (small-angle approx.) | ° | 0.0290 |
| `train_sx`, `train_sy`, `train_sz` | RMSE per axis (training/GCP) | m | 0.00295, 0.01283, ... |
| `valid_sx`, `valid_sy`, `valid_sz` | RMSE per axis (validation) | m | 0.01324, 0.01687, ... |
| `mode` | Input mode (`GeoPOINT` or `External`) | — | GeoPOINT |
| `n_total` | Total number of input points | — | 50 |
| `n_gcp` | Number of training points (Ground Control Points) | — | 5 |
| `n_valid` | Number of validation points | — | 45 |

**Interpretation tips:**
- If `detR ≈ 1.0` and rotation angles are small, the transformation is likely orthogonal and suitable for standard GIS use
- Compare `train_sx/sy/sz` with `valid_sx/sy/sz`: if validation much worse, the model may be overfitting
- `drift_col_10km_m` tells you practical accuracy at regional scales; check if it meets your task requirements

---

### `results_past.json` — Structured Result Record

Complete JSON export of transformation metadata, parameters, orthogonality checks, and RMSE diagnostics.

**Sample structure:**

```json
{
  "mode": "GeoPOINT",
  "n_total": 50,
  "n_gcp": 5,
  "n_valid": 45,
  "R": [
    [0.8066, -0.5861, 0.0783],
    [0.5909, 0.8030, -0.0786],
    [-0.0168, 0.1093, 0.9937]
  ],
  "t": [-2.37, -13.79, -2.92],
  "orthogonality": {
    "dots": {
      "c1·c2": -3.9782e-05,
      "c1·c3": 4.9830e-05,
      "c2·c3": -0.0002317
    },
    "norms": {
      "||c1||^2": 1.00002,
      "||c2||^2": 1.00017,
      "||c3||^2": 0.99966
    },
    "detR": 0.99999
  },
  "rotation_metrics": {
    "angle_deg": 0.0290,
    "angle_rad": 5.0676e-04,
    "drift_angle_10km_m": 5.07,
    "drift_col_10km_m": 1.68,
    "detR": 0.99999,
    "fro_error": 5.0676e-04
  },
  "rmse": {
    "training": {
      "sx": 0.00295,
      "sy": 0.01283,
      "sz": 0.00040
    },
    "validation": {
      "sx": 0.01324,
      "sy": 0.01687,
      "sz": 0.01334
    }
  }
}
```

### Use cases:
- Automated batch processing of multiple datasets
- Integration with dashboards or web applications
- Reproducibility archival
- Export to external tools (QGIS plugins, surveying software)


### `*_transformed.csv` Files — Per-Point Predictions

Five CSV files (one per method: PAST_raw, PAST_SVD, LSM_rigid, LSM_sim, HYB)
containing per-point transformed coordinates for all input points.

**Structure** (same for all five files):

| **Column** | **Description** | **Units** | **Example** |
|------------|-----------------|-----------|------------|
| `X_src` | Source (local) X coordinate | m | 1234.567 |
| `Y_src` | Source (local) Y coordinate | m | 5678.901 |
| `Z_src` | Source (local) Z coordinate | m | 45.123 |
| `X_pred` | Predicted (transformed) X coordinate | m | 1256.789 |
| `Y_pred` | Predicted (transformed) Y coordinate | m | 5645.234 |
| `Z_pred` | Predicted (transformed) Z coordinate | m | 47.891 |
| `X_ref` | Reference (global) X coordinate | m | 1256.801 |
| `Y_ref` | Reference (global) Y coordinate | m | 5645.210 |
| `Z_ref` | Reference (global) Z coordinate | m | 47.885 |

**Sample data (first 3 rows):**

```csv
X_src,Y_src,Z_src,X_pred,Y_pred,Z_pred,X_ref,Y_ref,Z_ref
1234.567,5678.901,45.123,1256.789,5645.234,47.891,1256.801,5645.210,47.885
1245.234,5689.456,46.234,1267.456,5655.789,48.902,1267.468,5655.765,48.896
1255.901,5699.012,47.345,1278.123,5666.345,49.913,1278.135,5666.321,49.907
```

**Use cases:**

1. GIS Import: Load CSV directly into QGIS, ArcGIS, or other GIS software
2. Error Analysis: Compute per-point residuals (X_pred − X_ref) to identify problematic points
3. Quality Control: Visualize prediction errors on maps to spot systematic biases
4. Batch Processing: Use Python/R scripts to compare methods across multiple datasets
5. Archive: Keep per-point records for reproducibility and audits


**Example Python analysis:**
```python
import pandas as pd
import numpy as np

# Load one of the transformed files
df = pd.read_csv('output_data/PAST_SVD_transformed.csv')

# Compute per-point 3D error
df['error_3d'] = np.sqrt(
    (df['X_pred'] - df['X_ref'])**2 +
    (df['Y_pred'] - df['Y_ref'])**2 +
    (df['Z_pred'] - df['Z_ref'])**2
)

# Summary statistics
print(f"Mean 3D error: {df['error_3d'].mean():.4f} m")
print(f"Median 3D error: {df['error_3d'].median():.4f} m")
print(f"Max 3D error: {df['error_3d'].max():.4f} m")
print(f"Std deviation: {df['error_3d'].std():.4f} m")

# Find problematic points
outliers = df[df['error_3d'] > df['error_3d'].quantile(0.95)]
print(f"\nPoints in top 5% error:\n{outliers[['X_src', 'Y_src', 'Z_src', 'error_3d']]}")
```

---

## Appendices

### Appendix 1: 1D Perceptron Learning Analogy

Educational section demonstrating how a single-layer perceptron learns to map 1D coordinates.
This simplified model illustrates the core concept behind PAST's 3D learning approach.

**Purpose:**
- Visualize gradient descent optimization step-by-step
- Show how weights and biases converge to an optimal mapping
- Understand loss function behavior and learning rate effects
- Bridge between pure mathematics and practical implementation

**What you'll see:**
- Interactive plot of training loss over iterations
- Per-iteration weight/bias updates
- Convergence analysis (when does learning plateau?)
- Comparison with analytical solution (if available)

**Best for:**
- Learning about gradient-based optimization
- Understanding PAST fundamentals without 3D complexity
- Teaching or presenting to non-technical audiences
- Debugging PAST behavior (does 3D learning follow same patterns?)

---

### Appendix 2: Consolidated Statistics & Comparison

Comprehensive side-by-side comparison of all five transformation methods 
(PAST_raw, PAST_SVD, LSM_rigid, LSM_sim, HYB) evaluated on both training and validation data.

**Metrics computed:**

| **Metric** | **Description** |
|------------|-----------------|
| **RMSE (per axis)** | Root Mean Square Error in X, Y, Z directions [m] |
| **RMST (3D aggregate)** | 3D spatial residual: $\sqrt{\text{RMSE}_x^2 + \text{RMSE}_y^2 + \text{RMSE}_z^2}$ [m] |
| **NRMSE** | Normalized RMSE (RMSE / data range) [—] |
| **3D per-point error** | Euclidean distance: $\sqrt{(X_{pred} - X_{ref})^2 + (Y_{pred} - Y_{ref})^2 + (Z_{pred} - Z_{ref})^2}$ [m] |
| **Orthogonality checks** | Frobenius norm, column norm deviations, determinant error |

**Output tables:**

1. **Training metrics table**: RMSE/RMST/NRMSE for each method on GCP points
2. **Validation metrics table**: Same metrics on held-out validation points
3. **Statistical summary**: Mean, median, max, std of per-point 3D errors
4. **Orthogonality report**: Tolerance levels (strict/practical/lenient) with pass/fail indicators

**Visualizations:**

- **Histograms** (per method): Distribution of 3D errors on training vs validation data
- **Box plots**: Quartile comparison across all five methods side-by-side
- **Text summary**: Practical interpretation (e.g., "PAST_SVD is 3% better on validation")

**Key insights from this section:**

- Compare training vs validation error to detect overfitting
- See which method performs best for your specific task
- Understand trade-offs: PAST_raw may excel on training but PAST_SVD generalizes better
- Orthogonality checks help decide if a method is suitable for downstream GIS/surveying work

**Example output** (from sample dataset with N=50, n_gcp=5):

```json
Training metrics (GCP set, n=5):
PAST_raw: RMSE = [0.0030, 0.0128, 0.0004] m → RMST = 0.0131 m
PAST_SVD: RMSE = [0.0031, 0.0130, 0.0005] m → RMST = 0.0133 m
LSM_rigid: RMSE = [0.0214, 0.0315, 0.0186] m → RMST = 0.0411 m
LSM_sim: RMSE = [0.0032, 0.0128, 0.0004] m → RMST = 0.0131 m
HYB: RMSE = [0.0030, 0.0127, 0.0004] m → RMST = 0.0130 m

Validation metrics (hold-out set, n=45):
PAST_raw: 3D error mean = 0.0225 m, median = 0.0198 m, max = 0.0456 m
PAST_SVD: 3D error mean = 0.0218 m, median = 0.0191 m, max = 0.0449 m ← Best
LSM_rigid: 3D error mean = 0.0348 m, median = 0.0312 m, max = 0.0687 m
LSM_sim: 3D error mean = 0.0219 m, median = 0.0193 m, max = 0.0451 m
HYB: 3D error mean = 0.0217 m, median = 0.0190 m, max = 0.0447 m ← Best
```

Conclusion: PAST_SVD provides ~3% better generalization than PAST_raw,
validating the orthogonality constraint for this dataset.

**Best for:**

- **Method selection**: Choose the best transformation for your task
- **Quality assurance**: Verify that results meet your geodetic requirements
- **Publication**: Document robustness across multiple approaches
- **Reproducibility**: Archive detailed statistics for future reference
- **Benchmarking**: Compare PAST against classical methods (LSM_rigid, LSM_sim)

---

## Contributing

Contributions are welcome:  
- Bug fixes and improvements.  
- Additional error models or transformation options.  
- Examples of usage in geodetic or geospatial projects.  


## License

MIT License – see `LICENSE` file.


## Citation
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.18288695.svg)](https://doi.org/10.5281/zenodo.18288695)

